{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALEPH\n",
    "a PoC for conversational research environment with ADA and GPT3.5\n",
    "\n",
    "Sources:\n",
    "- How_to_format_inputs_to_ChatGPT_models: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
    "- Question_answering_using_embeddings: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: come up with complexity control of the API call (count the number of tokens, manage contexts and so on)\n",
    "# TODO: implement chat memory (sqlite3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai, pickle, tiktoken, ast\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re, sqlite3\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from datetime import date, datetime\n",
    "\n",
    "import pprint\n",
    "\n",
    "import utils as utl\n",
    "import params as prm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your own key\n",
    "# read secret key from secret file \n",
    "with open('secret.txt', 'r') as f:\n",
    "    openai.api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today date is:  2023-03-14\n"
     ]
    }
   ],
   "source": [
    "# Returns the current local date\n",
    "today = date.today()\n",
    "\n",
    "print(\"Today date is: \", today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_name = 'good_incontext_examples_for_gpt3'\n",
    "session_date = today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to user's database\n",
    "conn = utl.create_connection(prm.DB_PTH) # Create connection to the database\n",
    "\n",
    "utl.create_table(conn, prm.SESSION_TABLE_SQL) # Create table if it does not exist\n",
    "# Create interaction table for the session\n",
    "utl.create_table(conn, f\"CREATE TABLE IF NOT EXISTS interaction_{session_name} (session_name, interaction_type, text, embedding, num_tokens_oai)\")\n",
    "\n",
    "tables = utl.parse_tables(conn) # Grab tables to check if the table was created\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('session',), ('interaction_good_incontext_examples_for_gpt3',)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session: \"good_incontext_examples_for_gpt3\" inserted into the database\n"
     ]
    }
   ],
   "source": [
    "# DB INTERACTION\n",
    "# Write session data to the database\n",
    "conn = utl.create_connection(prm.DB_PTH) # Create connection to the database\n",
    "utl.insert_session(conn, session_name, session_date) # Insert session data  \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare document for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic for chapter capture\n",
    "# digit_word_rgx = r'^\\d+(\\.\\d+)*\\s+[A-Z].*$|Abstract'\n",
    "digit_word_rgx = r'^\\d+(\\.\\d+)*\\s[a-zA-Z]+.*$|Abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load context document and grab chapter names\n",
    "doc_name = 'What Makes Good InContext Examples for GPT3.pdf'\n",
    "\n",
    "pdf_text = extract_text(prm.IN_PTH / doc_name)\n",
    "\n",
    "# TODO: user should decide which split logic to use: random split (text be be cut into equal parts containing more-less the same number of tokens) or split by chapters (chapter inference is not always correct as pdfs are not always formated nicely). \n",
    "# TODO: Implement interface for split choice + equal chunking function\n",
    "chapters = utl.grab_chapters(pdf_text, matching_logic=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract',\n",
       " '3 keeps generating tokens until there is a special char-',\n",
       " '2 Method',\n",
       " '2.1 GPT-3 for In-Context Learning',\n",
       " '2.2 The Impact of In-Context Examples',\n",
       " '2.3 kNN-augmented In-Context Example',\n",
       " '3 Experimental Setup',\n",
       " '3.1 Sentence Embeddings for Retrieval',\n",
       " '3 on open-domain QA tasks. The EM score is',\n",
       " '3.2 Baseline Methods',\n",
       " '4 Experimental Results',\n",
       " '4.1 Sentiment Analysis',\n",
       " '2 dataset, the accuracy of kNNsst-2 is 92.46, which',\n",
       " '4.2 Table-to-text Generation',\n",
       " '4.3 Questing Answering',\n",
       " '64 nearest neighbors (10 for TriviaQA) to deter-',\n",
       " '3 to misinterpret the question as asking for a spe-',\n",
       " '64 examples.',\n",
       " '5 Analysis and Ablation Study',\n",
       " '5.1 Number of In-context Examples',\n",
       " '5.2 Size of Training Set for Retrieval',\n",
       " '3 to better answer the questions.',\n",
       " '5.3 Order of In-context Examples',\n",
       " '6 Related Work',\n",
       " '7 Conclusion']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what chapters got captured from the pdf\n",
    "chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19 chapters in the document\n"
     ]
    }
   ],
   "source": [
    "# TODO: interface for selecting chapters to be used for text fragmentation. \n",
    "chapters = [\n",
    "    'Abstract',\n",
    "    '1 Introduction',\n",
    "    '2 Method',\n",
    "    '2.1 GPT-3 for In-Context Learning',\n",
    "    '2.2 The Impact of In-Context Examples',\n",
    "    '2.3 kNN-augmented In-Context Example',\n",
    "    '3 Experimental Setup',\n",
    "    '3.1 Sentence Embeddings for Retrieval',\n",
    "    '3.2 Baseline Methods',\n",
    "    '4 Experimental Results',\n",
    "    '4.1 Sentiment Analysis',\n",
    "    '4.2 Table-to-text Generation',\n",
    "    '4.3 Questing Answering',\n",
    "    '5 Analysis and Ablation Study',\n",
    "    '5.1 Number of In-context Examples',\n",
    "    '5.2 Size of Training Set for Retrieval',\n",
    "    '5.3 Order of In-context Examples',\n",
    "    '6 Related Work',\n",
    "    '7 Conclusion'\n",
    "]\n",
    "chapters = [x.lower().strip() for x in chapters]\n",
    "print(f\"There are {len(chapters)} chapters in the document\")\n",
    "\n",
    "\n",
    "# Set a specific starting point of the document\n",
    "# start = 'abstract'\n",
    "start = chapters[0]\n",
    "\n",
    "start = start.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make this text pre-processing function\n",
    "# define the text to be parsed\n",
    "text = [] \n",
    "for line in pdf_text.split('\\n'):\n",
    "    # if len(line)<=1: #get rid of junk lines\n",
    "    #     continue\n",
    "    line = line.replace('\\t', ' ')\n",
    "    line = line.strip().lower()\n",
    "    text.append(line)\n",
    "\n",
    "text = ' '.join(text)\n",
    "# replace newline and tab characters with space\n",
    "text = text.replace('\\n', '')\n",
    "text = text.replace('\\t', '')\n",
    "text = re.sub(r'\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: turn it into a function that will get triggered whe user decides to chop the document by chapter names\n",
    "\n",
    "# Fragment the text according to the logic the user defined (currently - by chapters)\n",
    "\n",
    "# join the end strings with | to form chapter end regex pattern\n",
    "end_pattern = \"|\".join(chapters)\n",
    "\n",
    "# match text between chapter and any end string\n",
    "chapters_contents = {}\n",
    "for string in chapters:\n",
    "\n",
    "    pattern = rf\"{string}(.*?)(\" + end_pattern + \"|$)\"\n",
    "    pattern = re.compile(pattern)\n",
    "\n",
    "\n",
    "\n",
    "    # search for the pattern in the text\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    # if there is a match, extract the text between string and any end-string\n",
    "    if match:\n",
    "        # get the first group of the match object, which is the text between given chapter and any end string\n",
    "        result = match.group(1)\n",
    "\n",
    "        # print or save or do whatever you want with the result\n",
    "        chapters_contents[string] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: come up with test that checks if I grabbed all the chapters\n",
    "fetched_chapters = [x for x in chapters_contents.keys()] \n",
    "# compare element wise fetched_chapters with chapters\n",
    "[x for x in chapters if x not in fetched_chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' gpt-3 (brown et al., 2020) is a new breakthrough in nlp research. previously, nlp models are pre- trained on large quantities of data and ﬁne-tuned ∗work was done during an internship at microsoft dy- namics 365 ai. trial accuracy 1 94.6 2 95.0 3 95.8 4 93.9 5 86.9 table 1: results of gpt-3 on the task of sentiment analysis on the sst-2 dataset. five different in-context examples are randomly selected from the training set. we observe different contexts induce different accura- cies on the test set. on a speciﬁc task and dataset. what sets gpt-3 apart from other pre-trained language models is its impressive “in-context” few-shot learning ability. provided with a few in-context examples, gpt-3 is able to generalize to unseen cases without fur- ther ﬁne-tuning. this opens up many new tech- nological possibilities that are previously consid- ered unique to human. for example, nlp systems can be developed to expand emails, extract entities from text, generate code based on natural language instructions with a few demonstration examples. despite its powerful and versatile in-context learning ability, gpt-3 has some practical chal- lenges/ambiguities. the original paper (brown et al., 2020) utilizes task-relevant examples that are randomly sampled from the training set to con- struct the context. in practice, we observe that the performance of gpt-3 tends to ﬂuctuate with dif- ferent choices of in-context examples. as shown in table 1, the variance of the empirical results with distinct in-context examples can be signiﬁ- cant. the results are highly sensitive to the exam- ples. our work aims to carefully examine this is- sue to gain a deeper understanding on how to bet- ter select in-context examples to unleash gpt-3’s few-shot capabilities and further improve its per- formance. a brute-force approach would be to perform combinatorial search over the entire dataset. un- fortunately, this strategy is computationally ex- pensive and thus impractical in many cases. to this end, we investigate the inﬂuences of employ- ing different in-context examples on the empiri- cal results. interestingly, we found that the in- context examples that are closer to the test sample in the embedding space consistently give rise to stronger performance (relative to the farther ones). inspired by this observation and the recent success of retrieval-augmented models (hashimoto et al., 2018), we propose to utilize nearest neighbors of a given test sample (among all the training instances available) as the corresponding in-context exam- ples. the retrieved examples, along with the test sample, are provided to gpt-3 for the ﬁnal predic- tion. to verify the effectiveness of the proposed method, we evaluate it on several natural language understanding and generation tasks, including sen- timent analysis, table-to-text generation and open- domain question answering. it is observed that the retrieval-based in-context examples unleash the few-shot capabilities of gpt-3 much more ef- fectively than a random sampling baseline. even with a smaller number of in-context examples, the proposed strategy empowers gpt-3 to achieve stronger performance. moreover, we ﬁnd that the speciﬁc sentence encoders employed for the re- trieval procedure play a critical role. thus, an ex- tensive exploration regarding different pre-trained encoders is conducted, and it is shown that en- coders ﬁne-tuned on natural language matching tasks serve as more effective in-context examples selector on the qa task. detailed analysis and case study further validate the effectiveness of pro- posed methods. in summary, our contributions in this paper are as follows: i) to the best of our knowledge, we take a ﬁrst step towards understanding the sensitivity of gpt- 3’s few-shot capabilities with respect to the selec- tion of in-context examples; ii) to alleviate the sensitivity issue, an ad- ditional retrieval module is introduced to ﬁnd semantically-similar in-context examples of a test instance to construct its corresponding input, which greatly outperforms the baseline based on random sampled examples; iii) ﬁne-tuning the retrieval model on task- related dataset(s) leads to even stronger empirical results with gpt-3; iv) the performance of gpt-3 improves as the number of examples available for retrieval in- creases. figure 1: the ﬁgure above shows how to perform in- context learning with a language model. three in- context examples and the test prompt are concatenated as a single string input for gpt-3, with a special charac- ter ”\\\\n” inserted between two adjacent examples. gpt- 3 keeps generating tokens until there is a special char- acter ”\\\\n”. '"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually inspect some chapters\n",
    "chapters_contents['1 introduction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abstract</th>\n",
       "      <td>gpt-3 (brown et al., 2020) has attracted lots...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 introduction</th>\n",
       "      <td>gpt-3 (brown et al., 2020) is a new breakthro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 method</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1 gpt-3 for in-context learning</th>\n",
       "      <td>the in-context learning scenario of gpt-3 can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.2 the impact of in-context examples</th>\n",
       "      <td>given the observation that the empirical resu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.3 knn-augmented in-context example</th>\n",
       "      <td>selection based on the ﬁndings above, we prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 experimental setup</th>\n",
       "      <td>we apply the knn in-context selection method ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.1 sentence embeddings for retrieval</th>\n",
       "      <td>to retrieve semantically-similar training ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.2 baseline methods</th>\n",
       "      <td>random sampling for each test sentence, we ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 experimental results</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.1 sentiment analysis</th>\n",
       "      <td>we ﬁrst evaluate kate on the sentiment anal- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.2 table-to-text generation</th>\n",
       "      <td>we utilize the totto dataset to evaluate kate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.3 questing answering</th>\n",
       "      <td>we also evaluate kate on the open-domain qa t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 analysis and ablation study</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1 number of in-context examples</th>\n",
       "      <td>we ﬁrst investigate the impact of the number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.2 size of training set for retrieval</th>\n",
       "      <td>we further examine how the size of the traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.3 order of in-context examples</th>\n",
       "      <td>moreover, we explore how the order of in-cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 related work</th>\n",
       "      <td>pre-trained language models nlp systems have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 conclusion</th>\n",
       "      <td>this work presented a ﬁrst step towards inves...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 contents\n",
       "abstract                                 gpt-3 (brown et al., 2020) has attracted lots...\n",
       "1 introduction                           gpt-3 (brown et al., 2020) is a new breakthro...\n",
       "2 method                                                                                 \n",
       "2.1 gpt-3 for in-context learning        the in-context learning scenario of gpt-3 can...\n",
       "2.2 the impact of in-context examples    given the observation that the empirical resu...\n",
       "2.3 knn-augmented in-context example     selection based on the ﬁndings above, we prop...\n",
       "3 experimental setup                     we apply the knn in-context selection method ...\n",
       "3.1 sentence embeddings for retrieval    to retrieve semantically-similar training ins...\n",
       "3.2 baseline methods                     random sampling for each test sentence, we ra...\n",
       "4 experimental results                                                                   \n",
       "4.1 sentiment analysis                   we ﬁrst evaluate kate on the sentiment anal- ...\n",
       "4.2 table-to-text generation             we utilize the totto dataset to evaluate kate...\n",
       "4.3 questing answering                   we also evaluate kate on the open-domain qa t...\n",
       "5 analysis and ablation study                                                            \n",
       "5.1 number of in-context examples        we ﬁrst investigate the impact of the number ...\n",
       "5.2 size of training set for retrieval   we further examine how the size of the traini...\n",
       "5.3 order of in-context examples         moreover, we explore how the order of in-cont...\n",
       "6 related work                           pre-trained language models nlp systems have ...\n",
       "7 conclusion                             this work presented a ﬁrst step towards inves..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab contents into a dataframe\n",
    "chapter_contents_df = pd.DataFrame(chapters_contents, index=['contents'])\n",
    "chapter_contents_df = chapter_contents_df.T\n",
    "chapter_contents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a token count column\n",
    "chapter_contents_df['num_tokens_oai'] = chapter_contents_df['contents'].apply(\n",
    "    lambda x: utl.num_tokens_from_messages([{'message': x}])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For instances with token count > token_thres, split them so they fit model threshold so we could get their embeddings\n",
    "# TODO: make it split actually by tokens, not by characters\n",
    "\n",
    "token_thres = 1000\n",
    "\n",
    "chapter_contents_df['split_factor'] = 1\n",
    "chapter_contents_df.loc[chapter_contents_df['num_tokens_oai'] > token_thres, 'split_factor'] = round(chapter_contents_df['num_tokens_oai']/token_thres,0)\n",
    "\n",
    "\n",
    "chapter_contents_df['contents_split'] = chapter_contents_df.apply(\n",
    "    lambda x: utl.split_contents(x), axis=1)\n",
    "\n",
    "\n",
    "chapter_contents_long_df = chapter_contents_df.explode(\n",
    "    column='contents_split'\n",
    ")[['contents_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents_split</th>\n",
       "      <th>num_tokens_oai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abstract</th>\n",
       "      <td>gpt-3 (brown et al., 2020) has attracted lots...</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 introduction</th>\n",
       "      <td>gpt-3 (brown et al., 2020) is a new breakthro...</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 method</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1 gpt-3 for in-context learning</th>\n",
       "      <td>the in-context learning scenario of gpt-3 can...</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.2 the impact of in-context examples</th>\n",
       "      <td>given the observation that the empirical resu...</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.3 knn-augmented in-context example</th>\n",
       "      <td>selection based on the ﬁndings above, we prop...</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 experimental setup</th>\n",
       "      <td>we apply the knn in-context selection method ...</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.1 sentence embeddings for retrieval</th>\n",
       "      <td>to retrieve semantically-similar training ins...</td>\n",
       "      <td>869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.2 baseline methods</th>\n",
       "      <td>random sampling for each test sentence, we ra...</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 experimental results</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.1 sentiment analysis</th>\n",
       "      <td>we ﬁrst evaluate kate on the sentiment anal- ...</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.2 table-to-text generation</th>\n",
       "      <td>we utilize the totto dataset to evaluate kate...</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.3 questing answering</th>\n",
       "      <td>we also evaluate kate on the open-domain qa t...</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 analysis and ablation study</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1 number of in-context examples</th>\n",
       "      <td>we ﬁrst investigate the impact of the number ...</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.2 size of training set for retrieval</th>\n",
       "      <td>we further examine how the size of the traini...</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.3 order of in-context examples</th>\n",
       "      <td>moreover, we explore how the order of in-cont...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 related work</th>\n",
       "      <td>pre-trained language models nlp systems have ...</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 conclusion</th>\n",
       "      <td>this work presented a ﬁrst step towards inves...</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           contents_split  \\\n",
       "abstract                                 gpt-3 (brown et al., 2020) has attracted lots...   \n",
       "1 introduction                           gpt-3 (brown et al., 2020) is a new breakthro...   \n",
       "2 method                                                                                    \n",
       "2.1 gpt-3 for in-context learning        the in-context learning scenario of gpt-3 can...   \n",
       "2.2 the impact of in-context examples    given the observation that the empirical resu...   \n",
       "2.3 knn-augmented in-context example     selection based on the ﬁndings above, we prop...   \n",
       "3 experimental setup                     we apply the knn in-context selection method ...   \n",
       "3.1 sentence embeddings for retrieval    to retrieve semantically-similar training ins...   \n",
       "3.2 baseline methods                     random sampling for each test sentence, we ra...   \n",
       "4 experimental results                                                                      \n",
       "4.1 sentiment analysis                   we ﬁrst evaluate kate on the sentiment anal- ...   \n",
       "4.2 table-to-text generation             we utilize the totto dataset to evaluate kate...   \n",
       "4.3 questing answering                   we also evaluate kate on the open-domain qa t...   \n",
       "5 analysis and ablation study                                                               \n",
       "5.1 number of in-context examples        we ﬁrst investigate the impact of the number ...   \n",
       "5.2 size of training set for retrieval   we further examine how the size of the traini...   \n",
       "5.3 order of in-context examples         moreover, we explore how the order of in-cont...   \n",
       "6 related work                           pre-trained language models nlp systems have ...   \n",
       "7 conclusion                             this work presented a ﬁrst step towards inves...   \n",
       "\n",
       "                                        num_tokens_oai  \n",
       "abstract                                           338  \n",
       "1 introduction                                    1053  \n",
       "2 method                                             7  \n",
       "2.1 gpt-3 for in-context learning                  489  \n",
       "2.2 the impact of in-context examples              377  \n",
       "2.3 knn-augmented in-context example               833  \n",
       "3 experimental setup                               246  \n",
       "3.1 sentence embeddings for retrieval              869  \n",
       "3.2 baseline methods                               416  \n",
       "4 experimental results                               7  \n",
       "4.1 sentiment analysis                             373  \n",
       "4.2 table-to-text generation                       424  \n",
       "4.3 questing answering                            1399  \n",
       "5 analysis and ablation study                        7  \n",
       "5.1 number of in-context examples                  178  \n",
       "5.2 size of training set for retrieval             768  \n",
       "5.3 order of in-context examples                   394  \n",
       "6 related work                                    1052  \n",
       "7 conclusion                                       644  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a token count column\n",
    "chapter_contents_long_df['num_tokens_oai'] = chapter_contents_long_df['contents_split'].apply(\n",
    "    lambda x: utl.num_tokens_from_messages([{'message': x}])\n",
    ")\n",
    "\n",
    "chapter_contents_long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_contents_long_df['text'] = \"CHAPTER: \" +  chapter_contents_long_df.index + \" CONTENT: \" + chapter_contents_long_df['contents_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop contents_split column\n",
    "chapter_contents_long_df = chapter_contents_long_df.drop(columns=['contents_split'])\n",
    "# Reset index so chapter names are stored in columns\n",
    "chapter_contents_long_df = chapter_contents_long_df.reset_index()\n",
    "# Rename index column to chapter\n",
    "chapter_contents_long_df = chapter_contents_long_df.rename(columns={'index': 'chapter'})\n",
    "# Add session_name column\n",
    "chapter_contents_long_df['session_name'] = session_name\n",
    "## Add interaction type column\n",
    "chapter_contents_long_df['interaction_type'] = 'source'\n",
    "## Drop rows where num_tokens_oai is less than 100\n",
    "chapter_contents_long_df = chapter_contents_long_df[chapter_contents_long_df['num_tokens_oai'] > 50].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>num_tokens_oai</th>\n",
       "      <th>text</th>\n",
       "      <th>session_name</th>\n",
       "      <th>interaction_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abstract</td>\n",
       "      <td>338</td>\n",
       "      <td>CHAPTER: abstract CONTENT:  gpt-3 (brown et al...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 introduction</td>\n",
       "      <td>1053</td>\n",
       "      <td>CHAPTER: 1 introduction CONTENT:  gpt-3 (brown...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.1 gpt-3 for in-context learning</td>\n",
       "      <td>489</td>\n",
       "      <td>CHAPTER: 2.1 gpt-3 for in-context learning CON...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2 the impact of in-context examples</td>\n",
       "      <td>377</td>\n",
       "      <td>CHAPTER: 2.2 the impact of in-context examples...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.3 knn-augmented in-context example</td>\n",
       "      <td>833</td>\n",
       "      <td>CHAPTER: 2.3 knn-augmented in-context example ...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 experimental setup</td>\n",
       "      <td>246</td>\n",
       "      <td>CHAPTER: 3 experimental setup CONTENT:  we app...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.1 sentence embeddings for retrieval</td>\n",
       "      <td>869</td>\n",
       "      <td>CHAPTER: 3.1 sentence embeddings for retrieval...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.2 baseline methods</td>\n",
       "      <td>416</td>\n",
       "      <td>CHAPTER: 3.2 baseline methods CONTENT:  random...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.1 sentiment analysis</td>\n",
       "      <td>373</td>\n",
       "      <td>CHAPTER: 4.1 sentiment analysis CONTENT:  we ﬁ...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.2 table-to-text generation</td>\n",
       "      <td>424</td>\n",
       "      <td>CHAPTER: 4.2 table-to-text generation CONTENT:...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.3 questing answering</td>\n",
       "      <td>1399</td>\n",
       "      <td>CHAPTER: 4.3 questing answering CONTENT:  we a...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.1 number of in-context examples</td>\n",
       "      <td>178</td>\n",
       "      <td>CHAPTER: 5.1 number of in-context examples CON...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.2 size of training set for retrieval</td>\n",
       "      <td>768</td>\n",
       "      <td>CHAPTER: 5.2 size of training set for retrieva...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.3 order of in-context examples</td>\n",
       "      <td>394</td>\n",
       "      <td>CHAPTER: 5.3 order of in-context examples CONT...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6 related work</td>\n",
       "      <td>1052</td>\n",
       "      <td>CHAPTER: 6 related work CONTENT:  pre-trained ...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7 conclusion</td>\n",
       "      <td>644</td>\n",
       "      <td>CHAPTER: 7 conclusion CONTENT:  this work pres...</td>\n",
       "      <td>good_incontext_examples_for_gpt3</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   chapter  num_tokens_oai  \\\n",
       "0                                 abstract             338   \n",
       "1                           1 introduction            1053   \n",
       "3        2.1 gpt-3 for in-context learning             489   \n",
       "4    2.2 the impact of in-context examples             377   \n",
       "5     2.3 knn-augmented in-context example             833   \n",
       "6                     3 experimental setup             246   \n",
       "7    3.1 sentence embeddings for retrieval             869   \n",
       "8                     3.2 baseline methods             416   \n",
       "10                  4.1 sentiment analysis             373   \n",
       "11            4.2 table-to-text generation             424   \n",
       "12                  4.3 questing answering            1399   \n",
       "14       5.1 number of in-context examples             178   \n",
       "15  5.2 size of training set for retrieval             768   \n",
       "16        5.3 order of in-context examples             394   \n",
       "17                          6 related work            1052   \n",
       "18                            7 conclusion             644   \n",
       "\n",
       "                                                 text  \\\n",
       "0   CHAPTER: abstract CONTENT:  gpt-3 (brown et al...   \n",
       "1   CHAPTER: 1 introduction CONTENT:  gpt-3 (brown...   \n",
       "3   CHAPTER: 2.1 gpt-3 for in-context learning CON...   \n",
       "4   CHAPTER: 2.2 the impact of in-context examples...   \n",
       "5   CHAPTER: 2.3 knn-augmented in-context example ...   \n",
       "6   CHAPTER: 3 experimental setup CONTENT:  we app...   \n",
       "7   CHAPTER: 3.1 sentence embeddings for retrieval...   \n",
       "8   CHAPTER: 3.2 baseline methods CONTENT:  random...   \n",
       "10  CHAPTER: 4.1 sentiment analysis CONTENT:  we ﬁ...   \n",
       "11  CHAPTER: 4.2 table-to-text generation CONTENT:...   \n",
       "12  CHAPTER: 4.3 questing answering CONTENT:  we a...   \n",
       "14  CHAPTER: 5.1 number of in-context examples CON...   \n",
       "15  CHAPTER: 5.2 size of training set for retrieva...   \n",
       "16  CHAPTER: 5.3 order of in-context examples CONT...   \n",
       "17  CHAPTER: 6 related work CONTENT:  pre-trained ...   \n",
       "18  CHAPTER: 7 conclusion CONTENT:  this work pres...   \n",
       "\n",
       "                        session_name interaction_type  \n",
       "0   good_incontext_examples_for_gpt3           source  \n",
       "1   good_incontext_examples_for_gpt3           source  \n",
       "3   good_incontext_examples_for_gpt3           source  \n",
       "4   good_incontext_examples_for_gpt3           source  \n",
       "5   good_incontext_examples_for_gpt3           source  \n",
       "6   good_incontext_examples_for_gpt3           source  \n",
       "7   good_incontext_examples_for_gpt3           source  \n",
       "8   good_incontext_examples_for_gpt3           source  \n",
       "10  good_incontext_examples_for_gpt3           source  \n",
       "11  good_incontext_examples_for_gpt3           source  \n",
       "12  good_incontext_examples_for_gpt3           source  \n",
       "14  good_incontext_examples_for_gpt3           source  \n",
       "15  good_incontext_examples_for_gpt3           source  \n",
       "16  good_incontext_examples_for_gpt3           source  \n",
       "17  good_incontext_examples_for_gpt3           source  \n",
       "18  good_incontext_examples_for_gpt3           source  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter_contents_long_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get embeddings for each content piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_for_embed_df = chapter_contents_long_df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0039412"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the cost of running the model to get embeddings\n",
    "(chapter_contents_long_df['num_tokens_oai'].sum() / 1000) * 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out because I already ran this\n",
    "# If you want to get embeddings on a different file, \n",
    "# uncomment this and run it\n",
    "\n",
    "# Embed each chapter\n",
    "\n",
    "\n",
    "# rng = tqdm(range(0,len(contents_for_embed_df)))\n",
    "\n",
    "# contents_embedded = {}\n",
    "\n",
    "# for i in rng:\n",
    "#     txt_chapter = contents_for_embed_df.index[i]\n",
    "#     txt_list = contents_for_embed_df.iloc[i].to_list()\n",
    "\n",
    "#     txt_embed = utl.get_embedding(txt_list)\n",
    "\n",
    "\n",
    "\n",
    "#     # Join embeddings with context table\n",
    "#     contents_embedded[txt_chapter] = txt_embed\n",
    "# embeded_s = pd.Series(contents_embedded, index=contents_embedded.keys())\n",
    "\n",
    "# # Merge embeddings with chapter contents\n",
    "# chapter_contents_long_df['embedding'] = embeded_s\n",
    "# chapter_contents_long_df.head()\n",
    "\n",
    "# # Save embeddings\n",
    "# chapter_contents_long_df.to_csv(f'./data/{session_name}_embeded.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find most similar document embeddings to the question embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read embeddings\n",
    "chapter_contents_long_df = pd.read_csv(prm.D_PTH  / f'{session_name}_embeded.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context table for session: \"good_incontext_examples_for_gpt3\" created\n"
     ]
    }
   ],
   "source": [
    "# DB INTERACTION\n",
    "# Insert context into DB\n",
    "conn = utl.create_connection(prm.DB_PTH)\n",
    "utl.insert_context(conn, session_name, chapter_contents_long_df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSYSTEM INSTRUCTION: classify user query either as RECALL REQUEST, SOURCE QUERY or SUMMARY REQUEST. Recall request means that user asks you to recall some information from the conversation. Source query is when user asks a specific question about the source text. Summary request is when user asks general question about the text (for example - what's this text about). I want your response being either RECALL REQUEST, SOURCE QUERY, SUMMARY REQUEST\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To recognize previous interactions and to provide a summary of text we will implement a pre-call to gpt turbo that will classify the interaction type\n",
    "\n",
    "\"\"\"\n",
    "SYSTEM INSTRUCTION: classify user query either as RECALL REQUEST, SOURCE QUERY or SUMMARY REQUEST. Recall request means that user asks you to recall some information from the conversation. Source query is when user asks a specific question about the source text. Summary request is when user asks general question about the text (for example - what's this text about). I want your response being either RECALL REQUEST, SOURCE QUERY, SUMMARY REQUEST\n",
    "\"\"\"\n",
    "\n",
    "# Hold previous model response in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction type: \"user\" inserted into the database for session: \"good_incontext_examples_for_gpt3\"\n",
      "Interaction type: \"assistant\" inserted into the database for session: \"good_incontext_examples_for_gpt3\"\n",
      "'USER:'\n",
      "'what is your opinion on that?'\n",
      "'---------------------------------'\n",
      "'ASSISTANT:'\n",
      "('As an AI language model, I do not have personal opinions. However, based on '\n",
      " 'the information provided in the source, it appears that the approach of '\n",
      " 'retrieving semantically-similar examples to formulate a prompt has shown to '\n",
      " 'be effective in selecting in-context examples for sentiment analysis. It '\n",
      " 'also seems that fine-tuning on a similar task can benefit the performance of '\n",
      " 'the model. However, it is not clear from the given content about the '\n",
      " 'relevance or feasibility of using conversation history to build context.')\n",
      "'---------------------------------'\n",
      "'CONTEXT:'\n",
      "('SOURCE: CHAPTER: 4.1 sentiment analysis CONTENT:  we ﬁrst evaluate kate on '\n",
      " 'the sentiment anal- ysis task. the results are shown in table 4. it can be '\n",
      " 'observed that kate consistently pro- duces better performance relative to '\n",
      " 'the random selection baseline. notably, there is no vari- ance with the '\n",
      " 'obtained results since the same set of retrieved in-context examples are '\n",
      " 'employed. for the kate method, when a pre-trained sen- tence encoder is '\n",
      " 'ﬁne-tuned on nli or nli+sts- the performance slightly decreases. b datasets, '\n",
      " 'since the objectives of the imdb dataset and the nli+sts-b datasets are '\n",
      " 'different, this shows that ﬁne-tuning on a dissimilar task can hurt kate’s '\n",
      " 'performance. moreover, katenli+sts-b performs worse than katenli because the '\n",
      " 'sentence encoder has been further ﬁne-tuned on the sts-b dataset. in '\n",
      " 'contrast, katesst-2 obtains the best accuracy, showing that ﬁne-tuning on a '\n",
      " 'similar task can ben- eﬁt kate’s performance. to verify that the gains are '\n",
      " 'not merely from the retrieval step, we further compare kateroberta with the '\n",
      " 'knnroberta. it turns out that the performance of the knnroberta method is '\n",
      " 'similar to random guessing. this observation is consistent when one neighbor '\n",
      " 'or three neighbors are retrieved. notably, with the embeddings of the '\n",
      " 'roberta-large model ﬁne-tuned on the sst- 2 dataset, the accuracy of '\n",
      " 'knnsst-2 is 92.46, which is lower than that obtained with katesst-2. these '\n",
      " 'results suggest that the gpt-3 model is critical to the ﬁnal results, and '\n",
      " 'the retrieval module is com- plementary to gpt-3’s few-shot capabilities. ')\n",
      "'USER: how about when I want to ask the model to summarize the body of text?'\n",
      "('ASSISTANT: Based on the information provided in the abstract, the proposed '\n",
      " 'approach of retrieving examples that are semantically-similar to a test '\n",
      " 'sample to formulate its corresponding prompt has shown to be a more '\n",
      " 'effective strategy for judiciously selecting in-context examples compared to '\n",
      " 'random sampling. The approach consistently outperformed the random baseline '\n",
      " 'on several natural language understanding and generation benchmarks. '\n",
      " 'However, the abstract does not provide any specific information about the '\n",
      " 'feasibility or relevance of storing conversation history in an external '\n",
      " 'database and then parsing it to retrieve conversation fragments for building '\n",
      " 'context. Therefore, it is difficult to say whether this approach aligns with '\n",
      " 'the findings of the study.')\n"
     ]
    }
   ],
   "source": [
    "# POC - conversational interface\n",
    "\n",
    "## User input\n",
    "question = input(\"> \")\n",
    "\n",
    "\n",
    "\n",
    "## Fetch recal table so we can compare user input to embeddings saved in it and fetch the right context.\n",
    "recal_table = utl.fetch_recall_table(session_name)\n",
    "\n",
    "## Chop recall table to only include contexts for sources, user, or assistant\n",
    "recal_table_source = recal_table[recal_table['interaction_type'] == 'source']\n",
    "recal_table_user = recal_table[recal_table['interaction_type'] == 'user']\n",
    "recal_table_assistant = recal_table[recal_table['interaction_type'] == 'assistant']\n",
    "\n",
    "recal_embed_source = utl.convert_table_to_dct(recal_table_source)\n",
    "recal_embed_user = utl.convert_table_to_dct(recal_table_user)\n",
    "recal_embed_assistant = utl.convert_table_to_dct(recal_table_assistant)\n",
    "\n",
    "## Get the context from recal table that is the most similar to user input\n",
    "\n",
    "## Get SRC context\n",
    "if len(recal_embed_source) == 0:\n",
    "    recal_source = 'No context found'\n",
    "else:\n",
    "    recal_source_id = utl.order_document_sections_by_query_similarity(question, recal_embed_source)[0][1]\n",
    "    recal_source = recal_table.loc[recal_source_id]['text']\n",
    "## GET QRY context\n",
    "if len(recal_embed_user) == 0:\n",
    "    recal_user = 'No context found'\n",
    "else:\n",
    "    recal_user_id = utl.order_document_sections_by_query_similarity(question, recal_embed_user)[0][1]\n",
    "    recal_user = recal_table.loc[recal_user_id]['text']\n",
    "## GET RPL context\n",
    "if len(recal_embed_assistant) == 0:\n",
    "    recal_assistant = 'No context found'\n",
    "else:\n",
    "    recal_assistant_id = utl.order_document_sections_by_query_similarity(question, recal_embed_assistant)[0][1]\n",
    "    recal_assistant = recal_table.loc[recal_assistant_id]['text']\n",
    "\n",
    "\n",
    "\n",
    "## Grab chapter name if it exists, otherwise use session name\n",
    "## It will become handy when user wants to know from which chapter the context was taken\n",
    "source_chapter_val = recal_table.loc[recal_source_id]['chapter']\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Set-up system prompts. This is done once for the whole session and the setup depends on the type of assistant chosen by the user.\n",
    "# I comnment this out as this needs to be saved in the interaction table and I will implement this after user-assistant interactions are implemented.\n",
    "#\n",
    "# sys_message = [\n",
    "#     {'role': 'system', 'content': f\"You are a helpful assistant. You provide only factual information. If you do not know the answer, you say it.\"}\n",
    "#     ]\n",
    "# api_response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=sys_message\n",
    "# )\n",
    "###############################################################################\n",
    "\n",
    "## Form user message based on recaled context and user's input\n",
    "usr_message = [\n",
    "    {\n",
    "        'role': 'system', \n",
    "        'content': \"You are a helpful assistant. You provide only factual information. If you do not know the answer, you say it. I provide my input after INP tag. I will pass the context you will use in your answer. I encode it with following tags: SRC - sources we are talking about; QRY - one of previous inputs I passed to you in the conversation; RPL - one of your previous replies to my questions from the conversation.\"\n",
    "        },\n",
    "    {   \n",
    "        \"role\": \"user\", \n",
    "        \"content\": f\"SRC: {recal_source}. QRY: {recal_user}. RPL: {recal_assistant}. INP: {question}\"\n",
    "        }\n",
    "    ]\n",
    "# Grab call user content from messages alias\n",
    "usr_message_content = usr_message[0]['content']\n",
    "\n",
    "\n",
    "## Make API call with formed user message\n",
    "api_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=usr_message\n",
    ")\n",
    "\n",
    "\n",
    "# Open DB so the assistant can remember the conversation\n",
    "conn = utl.create_connection(prm.DB_PTH)\n",
    "# Insert user message into DB so we can use it for another user's input\n",
    "utl.insert_interaction(\n",
    "    conn, \n",
    "    session_name, \n",
    "    'user', \n",
    "    question\n",
    "    )\n",
    "# Insert model's response into DB so we can use it for another user's input\n",
    "utl.insert_interaction(\n",
    "    conn,\n",
    "    session_name,\n",
    "    'assistant',\n",
    "    api_response['choices'][0]['message']['content']\n",
    "    )\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Print CALL and RESPONSE\n",
    "pprint.pprint('USER:')\n",
    "pprint.pprint(question)\n",
    "pprint.pprint('---------------------------------')\n",
    "pprint.pprint('ASSISTANT:')\n",
    "pprint.pprint(api_response['choices'][0]['message']['content'])\n",
    "## Print additional context data:\n",
    "pprint.pprint('---------------------------------')\n",
    "pprint.pprint('CONTEXT:')\n",
    "pprint.pprint(f\"SOURCE: {recal_source}\")\n",
    "pprint.pprint(f\"USER: {recal_user}\")\n",
    "pprint.pprint(f\"ASSISTANT: {recal_assistant}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3289b5bb5c2761f56c4426244bcf49eab0c0b4a9e2bf0f472f2bef28cce96346"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
