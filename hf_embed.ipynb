{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nf/miniconda3/envs/alp-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "query = 'what was the involvment of army in artificial intelligence'\n",
    "\n",
    "text1 = \"\"\"\n",
    "    A latent space, also known as a latent feature space or embedding space, \n",
    "    is an embedding of a set of items within a manifold in which items resembling each other \n",
    "    are positioned closer to one another in the latent space. Position within the latent space \n",
    "    can be viewed as being defined by a set of latent variables that emerge from the resemblances \n",
    "    from the objects.\n",
    "    In most cases, the dimensionality of the latent space is chosen to be lower than the \n",
    "    dimensionality of the feature space from which the data points are drawn, making the \n",
    "    construction of a latent space an example of dimensionality reduction, which can also \n",
    "    be viewed as a form of data compression.[1] Latent spaces are usually fit via machine \n",
    "    learning, and they can then be used as feature spaces in machine learning models, including \n",
    "    classifiers and other supervised predictors.\n",
    "    The interpretation of the latent spaces of machine learning models is an active field of study, \n",
    "    but latent space interpretation is difficult to achieve. Due to the black-box nature of machine \n",
    "    learning models, the latent space may be completely unintuitive. Additionally, the latent space \n",
    "    may be high-dimensional, complex, and nonlinear, which may add to the difficulty of interpretation.[2] \n",
    "    Some visualization techniques have been developed to connect the latent space to the visual world, \n",
    "    but there is often not a direct connection between the latent space interpretation and the model itself. \n",
    "    Such techniques include t-distributed stochastic neighbor embedding (t-SNE), where the latent space is \n",
    "    mapped to two dimensions for visualization. Latent space distances lack physical units, so the interpretation \n",
    "    of these distances may depend on the application.[3]\n",
    "    A number of algorithms exist to create latent space embeddings given a set of data items and a similarity function.\n",
    "    \"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "    In physics and mathematics, the dimension of a mathematical space (or object) is informally defined as \n",
    "    the minimum number of coordinates needed to specify any point within it.[1][2] Thus, a line has a dimension \n",
    "    of one (1D) because only one coordinate is needed to specify a point on it – for example, the point at 5 on a \n",
    "    number line. A surface, such as the boundary of a cylinder or sphere, has a dimension of two (2D) because two \n",
    "    coordinates are needed to specify a point on it – for example, both a latitude and longitude are required to \n",
    "    locate a point on the surface of a sphere. A two-dimensional Euclidean space is a two-dimensional space on the \n",
    "    plane. The inside of a cube, a cylinder or a sphere is three-dimensional (3D) because three coordinates are \n",
    "    needed to locate a point within these spaces.\n",
    "    In classical mechanics, space and time are different categories and refer to absolute space and time. \n",
    "    That conception of the world is a four-dimensional space but not the one that was found necessary to \n",
    "    describe electromagnetism. The four dimensions (4D) of spacetime consist of events that are not absolutely \n",
    "    defined spatially and temporally, but rather are known relative to the motion of an observer. Minkowski space \n",
    "    first approximates the universe without gravity; the pseudo-Riemannian manifolds of general relativity describe \n",
    "    spacetime with matter and gravity. 10 dimensions are used to describe superstring theory (6D hyperspace + 4D), 11 \n",
    "    dimensions can describe supergravity and M-theory (7D hyperspace + 4D), and the state-space of quantum mechanics \n",
    "    is an infinite-dimensional function space.\n",
    "    The concept of dimension is not restricted to physical objects. High-dimensional spaces frequently occur in \n",
    "    mathematics and the sciences. They may be Euclidean spaces or more general parameter spaces or configuration spaces \n",
    "    such as in Lagrangian or Hamiltonian mechanics; these are abstract spaces, independent of the physical space \n",
    "    in which we live. \n",
    "    \"\"\"\n",
    "\n",
    "text3 = \"\"\"\n",
    "    The perceptron was intended to be a machine, rather than a program, and while its first implementation was in \n",
    "    software for the IBM 704, it was subsequently implemented in custom-built hardware as the \"Mark 1 perceptron\". \n",
    "    This machine was designed for image recognition: it had an array of 400 photocells, randomly connected to the \"neurons\". \n",
    "    Weights were encoded in potentiometers, and weight updates during learning were performed by electric motors.[2]: 193 \n",
    "    In a 1958 press conference organized by the US Navy, Rosenblatt made statements about the perceptron that caused a heated \n",
    "    controversy among the fledgling AI community; based on Rosenblatt's statements, The New York Times reported the perceptron \n",
    "    to be \"the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce \n",
    "    itself and be conscious of its existence.\"[5]\n",
    "    Although the perceptron initially seemed promising, it was quickly proved that perceptrons could not be trained to recognise \n",
    "    many classes of patterns. This caused the field of neural network research to stagnate for many years, before it was recognised \n",
    "    that a feedforward neural network with two or more layers (also called a multilayer perceptron) had greater processing power than \n",
    "    perceptrons with one layer (also called a single-layer perceptron). \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model token limit: 512\n",
      "Text1 token len: 377\n",
      "Text2 token len: 409\n",
      "Text3 token len: 409\n",
      "Similarity: tensor([[0.0072, 0.0236, 0.3592]])\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "print('Model token limit:',model.max_seq_length)\n",
    "tokens1 = tokenizer.encode(text1, truncation=False, return_tensors=\"pt\")\n",
    "tokens2 = tokenizer.encode(text2, truncation=False, return_tensors=\"pt\")\n",
    "tokens3 = tokenizer.encode(text3, truncation=False, return_tensors=\"pt\")\n",
    "print('Text1 token len:',len(tokens1[0]))\n",
    "print('Text2 token len:',len(tokens2[0]))\n",
    "print('Text3 token len:',len(tokens2[0]))\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "corpus = model.encode([text1,text2,text3])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a latent space, also known as a latent feature space or embedding space, is an embedding of a set of items within a manifold in which items resembling each other are positioned closer to one another in the latent space. position within the latent space can be viewed as being defined by a set of latent variables that emerge from the resemblances from the objects. in most cases, the dimensionality of the latent space is chosen to be lower than the dimensionality of the feature space from which the data points are drawn, making the construction of a latent space an example of dimensionality reduction, which can also be viewed as a form of data compression. [ 1 ] latent spaces are usually fit via machine learning, and they can then be used as feature spaces in machine learning models, including classifiers and other supervised predictors. the interpretation of the latent spaces of machine learning models is an active field of study, but latent space interpretation is difficult to achieve. due to the black - box nature of machine learning models, the latent space may be completely unintuitive. additionally, the latent space may be high - dimensional, complex, and nonlinear, which may add to the difficulty of interpretation. [ 2 ] some visualization techniques have been developed to connect the latent space to the visual world, but there is often not a direct connection between the latent space interpretation and the model itself. such techniques include t - distributed stochastic neighbor embedding ( t - sne ), where the latent space is mapped to two dimensions for visualization. latent space distances lack physical units, so the interpretation of these distances may depend on the application. [ 3 ] a number of algorithms exist to create latent space embeddings given a set of data items and a similarity function.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens1[0][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.6657, 0.6342, 0.7203]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken, openai\n",
    "model = 'text-embedding-ada-002'\n",
    "openai.api_key = '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'\n",
    "embedding1 = openai.Embedding.create(\n",
    "        input=text1, \n",
    "        model=model,\n",
    "        )['data'][0]['embedding']\n",
    "\n",
    "embedding2 = openai.Embedding.create(\n",
    "        input=text2, \n",
    "        model=model,\n",
    "        )['data'][0]['embedding']\n",
    "\n",
    "embedding3 = openai.Embedding.create(\n",
    "        input=text3, \n",
    "        model=model,\n",
    "        )['data'][0]['embedding']\n",
    "\n",
    "query_embedding = openai.Embedding.create(\n",
    "        input=query, \n",
    "        model=model,\n",
    "        )['data'][0]['embedding']\n",
    "\n",
    "corpus = [embedding1, embedding2, embedding3]\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, corpus))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
